{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC lab 13: PyTorch & Convolutional Neural Networks \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/school_chop.jpg\" style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks caused a major step forward in the performance of image recognition. These networks are mostly identical to standard neural networks, in which features are first learned through multiple (layers of) convolutions. Obtained features are subsequently used as the input for a standard neural network, often performing a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution is the iteration of a kernel with size $ M \\times N $ over a given input $ \\textbf{X} $, performing a 2D linear combination of the weights $ W $  of the kernel with the overlapping area of the input. For a normal convolution with single striding and no padding, the output $ y_{ij} $ is equal to:\n",
    "\n",
    "$$ y_{ij} = \\sum_{a=0}^{m-1} \\sum_{b=0}^{n-1} W_{ab} x_{(i+a)(j+b)} $$\n",
    "\n",
    "During a convolution the kernels slides over the input image to obtain a new image of outputs. The stride of a kernel defines the horizontal and vertical stepsize during iteration. Input data can be padded with multiple layers of a zero-filled border, increasing the output dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**convolution step with M,N = 3; stride = 2 and padding layer of 1**\n",
    "<img src=\"img/kern_mult.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**several other examples. An extended explanation on all types of convolutions can be found [here](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)**\n",
    "<img src=\"img/conv_ex_all.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network usually processes the image with **multiple sequential convolutions**. For each layer, the kernel is evaluated for **all channels** of the input data. It is important to understand that the output depth is correlated to the amount of different kernels, or features, every node has been initialized with. The kernel, although often depicted as only evaluating one layer, **a convolution actually has weights for all the different layers (channels)** when computing an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/conv_ex_3.gif\" style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/conv_schema.jpg\" style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic example of a convolutional neural network applies an activation function (e.g. **ReLU**) on the output of every convolutional layer, after which the activation signals are **maximum pooled**. Maximum pooling can reduces the amount of parameters present in a neural network, which reduces overfitting and computational burden. Maximum pooling is also initialized with specific arguments such as kernel size, stride and padding.\n",
    "\n",
    "<img src=\"img/max_pool.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layers of every convolutional neural netwerk always consist of several fully connected layers. The mathematical description of these layers were discussed in the previous PC lab. One can interpret the convolutional layers as the section of the netwerk in which features training is done (edges, contours, contrasts,...). These are used as inputs for the fully connected neural netwerk, which combines these features to train the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The softmax function** takes the n-dimensional output of the model and rescales these values to probabilities that sum up to one. It is typically used as the output layer for multiclass classification.\n",
    "\n",
    "$$ \\sigma(\\hat{y})_j = \\frac{e^{\\hat{y}_j}}{\\sum_{k=1}^{K} e^{\\hat{y}_k}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<img src=\"img/pytorch.png\" style=\"width:40%;float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a python package that provides two high-level features:\n",
    "\n",
    "- Tensor computation (like numpy) with **strong GPU acceleration**\n",
    "- **Deep Neural Networks** built on a tape-based autograd system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2>Installing PyTorch</h2>\n",
    "    <p>Run from terminal. In the same environment as scikit learn has been installed.</p>\n",
    "    <p>install on <b>LINUX</b>:</p>\n",
    "    <code>conda install pytorch-cpu torchvision-cpu -c pytorch</code>\n",
    "    <p>install on <b>WINDOWS</b>:</p>\n",
    "    <code>conda install pytorch-cpu -c pytorch\n",
    "    pip3 install torchvision</code>\n",
    "    <p>install on <b>MAC</b>:</p>\n",
    "    <code>conda install pytorch torchvision -c pytorch</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch can be approached in a similar way as numpy, where data is mainly exchanged through **Tensor** objects instead of **numpy** objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Switching between numpy arrays and tensors is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(4,3).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(np.zeros((4,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2>INFO</h2>\n",
    "    <p>PyTorch has many functions and objects that behave similar to the ones found in the numpy module. </p>\n",
    "    <ul>\n",
    "        <li><code>torch.zeros()</code></li>\n",
    "        <li><code>torch.ones()</code></li>\n",
    "        <li><code>torch.arange()</code></li>\n",
    "        <li><code>torch.Tensor.min()</code></li>\n",
    "        <li><code>torch.Tensor.shape</code></li>\n",
    "        <li>...</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>Structure of the exercises.</h1>\n",
    "    <p>During this PC-lab you will be introduced with an examplory workflow when training a predictive model for image recognition. More specifically, we will create a <b>convolutional neural network</b> to <b>recognize vehicles from animals</b> using the <b>CIFAR-10</b> dataset in <b>PyTorch</b>. Most of the code has been written out already, leaving time to build further upon the existing codebase to improve results or add interesting features.\n",
    "   \n",
    "</div>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/CIFAR10.jpg\" width='25%' align=\"right\">\n",
    "\n",
    "# CIFAR-10\n",
    "\n",
    "- 60000 $32 \\times 32$ colour images\n",
    "- 50000 training images + 10000 validation images\n",
    "- 10 classes $\\rightarrow$ 6000 images per class\n",
    "\n",
    "\n",
    "Collected for **MSc thesis** \n",
    "\n",
    "<ul>\n",
    "<a href=\"learning-features-2009-TR.pdf\">Learning Multiple Layers of Features from Tiny Images</a>, Alex Krizhevsky, 2009.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CIFAR-10** is next to **MNIST** an established computer-vision dataset which has been extensively used to evaluate the performance of a wide variety machine learning techniques. A list featuring some papers using CIFAR-10 can be found [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#CIFAR-10). To load the CIFAR-10 dataset into our environment, we can use the pytorch module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "validationset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "binary_classes = np.array(['vehicle', 'animal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are initialized through the use of class objects. Many of the functionalities necessary to create [**all types of neural networks**](http://www.asimovinstitute.org/neural-network-zoo/) have [**already been implemented**](http://pytorch.org/docs/master/nn.html). The following code creates a neural network with two convolutions and two fully connected layers. The network is identical to the neural net shown at the beginning of the notebook (with the car displayed), albeit with the addition of one fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise:</h2>\n",
    "        <p>Calculate the input nodes of the the first fully connected layer <code>self.fc1</code>. This is the first argument of the function <code>nn.Linear()</code>. The batch size does not influence this value. Notice that <code>x.view(-1, ...)</code> assigns the different batches to the first dimension.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 3 input image channels, 16 output channels, 5x5 square convolution\n",
    "        self.conv1 = nn.Conv2d(3, 16, (5,5))\n",
    "        # 16 input image channels, 32 output channels, 5x5 square convolution\n",
    "        self.conv2 = nn.Conv2d(16, 32, (5,5))\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(800, 64)\n",
    "           \n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window + RELU\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # Max pooling over a (2, 2) window + RELU\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # Flatten array \n",
    "        x = x.view(\"\"\"...\"\"\")\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the similarities between the creation of this neural network and the one created in the previous lab. To be able to call a variant of this network, we can choose to add parameter values to `def __init__(self, .....)`, such as kernel sizes, or the total amount of nodes in each fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch offers an easy way to obtain gradients, where no explicit calculation of the derivatives is required. This is done through `autograd.Variable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p><code>autograd.Variable</code> is the central class of the package. It wraps a <code>Tensor</code>, and supports nearly all operations defined on it. Once you finish your computation you can call <code>.backward()</code> and have all the gradients computed automatically.</p>\n",
    "    <p><code>Variable</code> and <code>Function</code> are interconnected and build up an acyclic graph, which encodes a complete history of computation. Each variable has a <code>.grad_fn</code> attribute that references a <code>Function</code> that has created the <code>Variable</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "z = y**4\n",
    "out = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching between a Variable and Tensor is also possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.data\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the model. The train_loader and validation_loader object is used to obtain data batches of fixed size. The `fit()` function trains the model for a specified amount of epochs while storing data obtained during training with the `logger()` class. `convert_to_binary_class()` is a custom function defined to convert the dataset into a binary class problem (vehicles <-> animals). This is merely done to make training faster. To train with the original classes, simply comment out the line `y_batch = convert_to_binary_class(y_batch)` twice, as found in the `fit()` function. Furthermore, the model has to be changed to have 10 outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_class(y_batch):\n",
    "    labels = []\n",
    "    for label in y_batch:\n",
    "        if label in [0,1,8,9]:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    \n",
    "    return torch.LongTensor(labels)\n",
    "       \n",
    "    \n",
    "def fit(model, train_loader, validation_loader, criterion, optimizer, log, epochs=20):\n",
    "    epoch = 0   # set starting epoch\n",
    "    while epoch<epochs:\n",
    "        print(\"\\nepoch {}\".format(epoch))\n",
    "        epoch +=1\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):   # iterate randomized batches\n",
    "            optimizer.zero_grad()\n",
    "            X_batch, y_batch = data\n",
    "            y_batch = convert_to_binary_class(y_batch)\n",
    "            X_batch, y_batch = Variable(X_batch), Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model.forward(X_batch)\n",
    "            loss = criterion(y_hat, y_batch)\n",
    "            loss.backward()   # Calculate gradient\n",
    "            optimizer.step()   # Update weights using defined optimizer\n",
    "            log.log_metrics(y_batch.data.numpy(), y_hat.data.numpy(), loss.item())\n",
    "            if (i%100 == 1):\n",
    "                log.output_metrics()\n",
    "        \n",
    "        # Repeat this process for the validation dataset\n",
    "        model.eval()\n",
    "        for i, data in enumerate(validation_loader, 0):\n",
    "            X_batch, y_batch = data\n",
    "            y_batch = convert_to_binary_class(y_batch)\n",
    "            X_batch, y_batch = Variable(X_batch), Variable(y_batch)\n",
    "            y_hat = model.forward(X_batch)\n",
    "            loss = criterion(y_hat, y_batch)\n",
    "            log.log_metrics(y_batch.data.numpy(), y_hat.data.numpy(), loss.item(), validation=True)\n",
    "        log.output_metrics(validation=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2><code>class logger()</code> </h2>\n",
    "    <p><code>logger()</code> has been implemented as a convenient way to store model metrics throughout the training process.  An object of this class can be created before training and is used for calculating, storing, printing and plotting model metrics. The object does only store the metrics as defined at initialization. Feel free to add yor own metrics to the class.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "class logger(object):\n",
    "    def __init__(self, metrics, max_i):\n",
    "        self.i = [0,0]\n",
    "        self.max_i = max_i\n",
    "        self.log_loss, self.log_auc, self.log_acc = False, False, False\n",
    "        self.metrics = {\"train\":{}, \"validation\":{}}\n",
    "        if \"loss\" in metrics:\n",
    "            self.log_loss = True \n",
    "            self.metrics[\"train\"].update({\"loss\":[0]})\n",
    "            self.metrics[\"validation\"].update({\"loss\":[0]})\n",
    "        if \"AUC\" in metrics:\n",
    "            self.log_auc = True\n",
    "            self.metrics[\"train\"].update({\"auc\":[0]})\n",
    "            self.metrics[\"validation\"].update({\"auc\":[0]})\n",
    "        if \"acc\" in metrics:\n",
    "            self.log_acc = True\n",
    "            self.metrics[\"train\"].update({\"acc\":[0]})\n",
    "            self.metrics[\"validation\"].update({\"acc\":[0]})\n",
    "        \n",
    "    def log_metrics(self, y_true, y_hat, loss, validation=False):\n",
    "        if validation:\n",
    "            sw = 1\n",
    "            sw_str = \"validation\"\n",
    "        else:\n",
    "            sw = 0\n",
    "            sw_str = \"train\"\n",
    "        self.i[sw] += 1\n",
    "            \n",
    "        if self.log_loss:\n",
    "            update = (self.metrics[sw_str][\"loss\"][-1]*(self.i[sw]-1)+loss)/self.i[sw]\n",
    "            self.metrics[sw_str][\"loss\"].append(update)\n",
    "\n",
    "        if self.log_auc:\n",
    "            auc = roc_auc_score(y_true, y_hat[:,1])\n",
    "            update = (self.metrics[sw_str][\"auc\"][-1]*(self.i[sw]-1)+auc)/self.i[sw]\n",
    "            self.metrics[sw_str][\"auc\"].append(update)\n",
    "        if self.log_acc:\n",
    "            acc = sum(y_hat.argmax(axis=1) == y_true)/len(y_true)\n",
    "            update = (self.metrics[sw_str][\"acc\"][-1]*(self.i[sw]-1)+acc)/self.i[sw]\n",
    "            self.metrics[sw_str][\"acc\"].append(update)\n",
    "            \n",
    "    def output_metrics(self, validation=False):\n",
    "        data = \"validation\" if validation else \"train\"\n",
    "        if validation:\n",
    "            print(\"\\n{}:\\t100.0%\".format(data), end=\"\")\n",
    "        else:\n",
    "            print(\"\\r{}:\".format(data), end=\"\")\n",
    "            print(\"\\t{:4.2f}%\".format((self.i[0]%self.max_i)/self.max_i*100), end=\"\")\n",
    "        for k, v in self.metrics[data].items():\n",
    "            print(\"\\t{}: {:5.3f}\".format(k, v[-1]), end=\"\")\n",
    "                \n",
    "    def plot_metrics(self):\n",
    "        fig, axes = plt.subplots(len(self.metrics[\"train\"]),\n",
    "                               2, figsize=(12,6*len(self.metrics[\"train\"])))\n",
    "        for i, dict_0 in enumerate(self.metrics.items()):\n",
    "            for j, dict_1 in enumerate(dict_0[1].items()):\n",
    "                axes[j,i].plot(range(len(dict_1[1][1:])),dict_1[1][1:])\n",
    "                axes[j,i].set_title(\"{} {}\".format(dict_0[0], dict_1[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded our data, defined our convolutional neural network, and created all the functions necessary for training, the actual training process can start. For this instance, we will use the cross entropy loss to optimize our model. Notice how `nn.CrossEntropyLoss()` incorporaties the softmax function on the inputs. [Adam](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam) is used to determine the step size using the gradient of the loss with respect to the weights. Adam is currently often considered the best optimizer for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "log = logger(metrics=[\"loss\",\"acc\", \"AUC\"], max_i = len(train_loader))\n",
    "fit(model, train_loader, validation_loader, criterion, optimizer, log, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we can plot saved metrics using the `logger.plot_metrics()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise:</h2>\n",
    "        <p><b>Evaluate</b> the metrics after 15-20 epochs. Should the model train longer?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise:</h2>\n",
    "        <p><b>Write out </b> some code to obtain the confusion matrix of the predictions on the validation data. create a function named `confusion_matrix_from_variable`</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load all data in two arrays\n",
    "\n",
    "y_hat_all = []\n",
    "y_true_all = []\n",
    "for i, data in enumerate(validation_loader, 0):\n",
    "    X_batch, y_batch = data\n",
    "    y_batch = convert_to_binary_class(y_batch)\n",
    "    X_batch, y_batch = Variable(X_batch), Variable(y_batch)\n",
    "    y_hat = model.forward(X_batch)\n",
    "    y_hat_all.append(y_hat)\n",
    "    y_true_all.append(y_batch)\n",
    "y_hat_all = torch.cat(y_hat_all)\n",
    "y_true_all = torch.cat(y_true_all)\n",
    "print(F.softmax(y_hat_all[:10]), y_true_all[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_from_variable(y_hat, y_true):\n",
    "    \"\"\"...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_from_variable(y_hat_all, y_true_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,20))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(validation_loader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images[:8]),)\n",
    "outputs = model.forward(Variable(images))\n",
    "predicted = torch.max(outputs.data, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_classes[predicted.numpy()[:8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_kernels(tensor, num_cols=6):\n",
    "    if not tensor.ndim==4:\n",
    "        raise Exception(\"assumes a 4D tensor\")\n",
    "    num_kernels = tensor.shape[0]\n",
    "    num_rows = 1+ num_kernels // num_cols\n",
    "    fig = plt.figure(figsize=(num_cols,2*num_rows))\n",
    "    for i in range(tensor.shape[0]):\n",
    "        ax1 = fig.add_subplot(2*num_rows,num_cols,i+1+tensor.shape[0])\n",
    "        ax1.imshow(np.mean(tensor[i],0))\n",
    "        ax1.axis('off')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "filters = model.modules\n",
    "body_model = [i for i in model.children()]\n",
    "conv1_weights = body_model[0].weight.data.numpy()\n",
    "plot_kernels(conv1_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Optional Exercise:</h2>\n",
    "        <p><b>Tweak and optimize</b> the model in any way you can. You can change to the architecture of the model, vary the hyperparameters and add any of the many optimization techniques found in literature. Feel free to seek me out and discuss with me.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>Further Reading</h1>\n",
    "    There are a plethora of blogs that describe or explain various elements of deep learning in an interesting and visual way. If you are interested in expanding your knowledge on these topics I highly recommend checking out <a href=http://colah.github.io/>http://colah.github.io/</a> and <a href=http://karpathy.github.io/>http://karpathy.github.io/</url>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "744px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "335px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
